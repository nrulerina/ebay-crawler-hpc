# -*- coding: utf-8 -*-
"""webcrawler_requests_beautifulsoup

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Qqxlbmzz5LPjD-8tln1n9OLbUcXcx-Q

## Installing required library
"""

# For MongoDB
!pip install "pymongo[srv]"==3.12
# For web scraping
!pip install beautifulsoup4
!pip install lxml

"""## Importing libraries"""

from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi
import requests
from bs4 import BeautifulSoup
import time
import random
from datetime import datetime
from collections import OrderedDict

"""## Connecting to MongoDB"""

uri = "mongodb+srv://qiaoying:abc12345abc@hpdp-ebay.l78ixmr.mongodb.net/?retryWrites=true&w=majority&appName=HPDP-eBay"

# Create a new client and connect to the server
client = MongoClient(uri, server_api=ServerApi('1'))

# Send a ping to confirm a successful connection
try:
    client.admin.command('ping')
    print("Pinged your deployment. You successfully connected to MongoDB!")
except Exception as e:
    print(e)

db = client['HPDP-eBay']
collection = db['eBay_CamerasPhoto']

"""## Setup and Initialization"""

brand_urls = {
    "Kodak": "https://www.ebay.com.my/b/Kodak-Cameras-Photo/625/bn_88436119?mag=1",
    "Polaroid": "https://www.ebay.com.my/b/Polaroid-Cameras-Photo/625/bn_88436036?mag=1",
    "Bell and Howell": "https://www.ebay.com.my/b/Bell-and-Howell-Cameras-Photo/625/bn_81241950?mag=1",
    "Leica": "https://www.ebay.com.my/b/Leica-Cameras-Photo/625/bn_81241860?mag=1",
    "Minolta": "https://www.ebay.com.my/b/Minolta-Cameras-Photo/625/bn_81241818?mag=1",
    "Canon": "https://www.ebay.com.my/b/Canon-Cameras-Photo/625/bn_88436096?mag=1",
    "Unbranded": "https://www.ebay.com.my/b/Cameras-Photo/625/bn_1865546?Brand=Unbranded&mag=1&rt=nc",
    "ZEISS": "https://www.ebay.com.my/b/ZEISS-Cameras-Photo/625/bn_88436088?mag=1",
    "Ansco": "https://www.ebay.com.my/b/Ansco-Cameras-Photo/625/bn_81241989?mag=1",
    "Nikon": "https://www.ebay.com.my/b/Nikon-Cameras-Photo/625/bn_81241923?mag=1",
    "Agfa": "https://www.ebay.com.my/b/Agfa-Cameras-Photo/625/bn_88436032?mag=1",
    "Keystone": "https://www.ebay.com.my/b/Keystone-Cameras-Photo/625/bn_88436114?mag=1",
    "Castle Films": "https://www.ebay.com.my/b/Cameras-Photo/625/bn_1865546?Brand=Castle%2520Films&mag=1&rt=nc",
    "Vivitar": "https://www.ebay.com.my/b/Vivitar-Cameras-Photo/625/bn_81241943?mag=1",
    "GE": "https://www.ebay.com.my/b/Cameras-Photo/625/bn_1865546?Brand=GE&mag=1&rt=nc",
    "Yashica": "https://www.ebay.com.my/b/Yashica-Cameras-Photo/625/bn_81241895?mag=1",
    "Revere": "https://www.ebay.com.my/b/Cameras-Photo/625/bn_1865546?Brand=Revere&mag=1&rt=nc",
    "PENTAX": "https://www.ebay.com.my/b/PENTAX-Cameras-Photo/625/bn_81241983?mag=1",
    "Eastman": "https://www.ebay.com.my/b/Eastman-Cameras-Photo/625/bn_81241858?mag=1",
    "Bolex": "https://www.ebay.com.my/b/Cameras-Photo/625/bn_1865546?Brand=Bolex&mag=1&rt=nc",
}

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36",
    "Accept-Language": "en-US, en;q=0.5",
}

csv_file_name = "brand.csv"
log_file_name = "brand_log.txt"
target_per_brand = 2000
max_empty_pages = 30


result_list = []
log_lines = []
total_overall = 0

"""## Scrape"""

start_time = datetime.now()
log_lines.append(f"Scraping started at {start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")

# Loop through each brand
for brand_name, brand_url in brand_urls.items():
    print(f"\nüîç Scraping brand: {brand_name}")
    page = 1
    empty_page_count = 0
    brand_result = []

    while len(brand_result) < target_per_brand:

        try:
            paged_url = f"{brand_url}&_pgn={page}"
            print(f"‚û°Ô∏è  Scraping page {page}... (Brand: {brand_name}...(Collected: {len(brand_result)})")
            response = requests.get(paged_url, headers=headers, timeout=10)
            print(f"üîó Requested URL: {response.url}")

            if response.status_code != 200:
                print(f"‚ùó Error {response.status_code} on page {page}. Skipping...")
                time.sleep(random.uniform(5, 10))
                continue

            soup = BeautifulSoup(response.text, "lxml")
            listings = soup.select("li[class^='brwrvr__item-card'][class*='brwrvr__item-card--list']")
            category_tag = soup.find("h1", class_="textual-display page-title")
            category = category_tag.text.strip() if category_tag else ""

            if not listings:
                empty_page_count += 1
                page += 1
                time.sleep(random.uniform(2, 4))
                continue
            else:
                empty_page_count = 0

            for listing in listings:
                title_tag = listing.find("h3", class_="textual-display bsig__title__text")
                price_tag = listing.find("span", class_="textual-display bsig__price bsig__price--displayprice")
                product_url_tag = listing.find("a")

                status_block = listing.find("span", class_="textual-display bsig__generic bsig__listingCondition")
                status_spans = status_block.find_all("span") if status_block else []
                product_status = status_spans[0].text.strip() if len(status_spans) >= 1 else ""

                shipping_tag = listing.find("span", class_="textual-display bsig__generic bsig__logisticsCost")
                shipping = shipping_tag.text.strip() if shipping_tag else ""

                if title_tag and price_tag and product_url_tag:
                    brand_result.append({
                        "category": category,
                        "title": title_tag.text.strip(),
                        "brand": brand_name,
                        "price": price_tag.text.strip(),
                        "status": product_status,
                        "shipping": shipping,
                        "link": product_url_tag["href"],
                        "scraped_at": datetime.utcnow()  # Add timestamp
                    })

                if len(brand_result) >= target_per_brand:
                    break

            page += 1
            time.sleep(random.uniform(2, 4))

        except Exception as e:
            error_msg = f"Error on page {page} for brand {brand_name}: {e}\n"
            print(error_msg)
            log_lines.append(error_msg)
            time.sleep(random.uniform(5, 10))
            continue

    result_list.extend(brand_result)
    total_overall += len(brand_result)
    log_lines.append(f"‚úÖ {len(brand_result)} records scraped for {brand_name} across {page - 1} pages\n")

"""## Save data to MongoDB"""

# Insert all scraped items into MongoDB
if result_list:
    collection.insert_many(result_list)
    print(f"{len(result_list)} items inserted into MongoDB!")
else:
    print("No items to insert.")

# Finalize log
end_time = datetime.now()
log_lines.append(f"\nTotal records scraped: {total_overall}")
log_lines.append(f"Scraping ended at {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
log_lines.append(f"Duration: {str(end_time - start_time)}")

with open(log_file_name, "w", encoding="utf-8") as log_file:
    log_file.write("\n".join(log_lines))

print(f"\n‚úÖ Finished scraping {total_overall} products. Log saved to '{log_file_name}' and data to '{csv_file_name}'.")