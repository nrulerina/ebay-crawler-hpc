# -*- coding: utf-8 -*-
"""WEB_CRAWLER-SELENIUM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10Tp_gB2XuRP3NPkIelQRoChpNwce71kp

#ADDTIONAL
"""

from google.colab import drive
drive.mount('/content/drive')

pip install selenium

"""#WEB SCRAPPING

##PRICE <65
"""

import csv
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import NoSuchElementException, TimeoutException
import time
import random

user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/113.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/112.0.1722.64",
]

def get_driver():
    user_agent = random.choice(user_agents)
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument(f"user-agent={user_agent}")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    service = Service()
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def scrape_ebay():
    base_url = "https://www.ebay.com.my/b/Consumer-Electronics/293/bn_1865552?_pgn={}&_udhi=65&mag=1&rt=nc"
    max_pages = 169
    driver = get_driver()
    all_data = []

    try:
        for page_num in range(1, max_pages + 1):
            url = base_url.format(page_num)
            print(f"\n--- Scraping Page {page_num} ---")
            driver.get(url)
            time.sleep(2 + random.uniform(0.5, 1.5))

            listings = driver.find_elements(By.CSS_SELECTOR, "div.brwrvr__item-card__signals")
            print(f"Found {len(listings)} listings")

            for listing in listings:
                try:
                    title = listing.find_element(By.CSS_SELECTOR, 'h3.textual-display.bsig__title__text').text
                except NoSuchElementException:
                    title = None

                try:
                    price = listing.find_element(By.CSS_SELECTOR, 'span.textual-display.bsig__price.bsig__price--displayprice').text
                except NoSuchElementException:
                    price = None

                try:
                    shippingfee = listing.find_element(By.CSS_SELECTOR, 'span.textual-display.bsig__generic.bsig__logisticsCost').text
                except NoSuchElementException:
                    shippingfee = None

                try:
                    conditions = listing.find_elements(By.CSS_SELECTOR, 'span.bsig__listingCondition > span')
                    texts = [c.text.strip() for c in conditions if c.text.strip() and c.text.strip() != "·"]
                except NoSuchElementException:
                    texts = []

                try:
                    link = listing.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')
                except NoSuchElementException:
                    link = None

                data = {
                    "title": title,
                    "price": price,
                    "shippingfee": shippingfee,
                    "condition": texts[0] if len(texts) > 0 else None,
                    "brand": texts[1] if len(texts) > 1 else None,
                    "link": link
                }

                all_data.append(data)

            time.sleep(random.uniform(2, 5))
    except TimeoutException as e:
        print(f"TimeoutException: {e}")
        driver.save_screenshot("timeout_error.png")
    except Exception as e:
        print(f"Unexpected error: {e}")
        driver.save_screenshot("general_error.png")
    finally:
        driver.quit()

        # Save to CSV
        keys = ["title", "price", "shippingfee", "condition", "brand", "link"]
        with open("ebay_listings.csv", "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            writer.writerows(all_data)
        print(f"\n✅ Data saved to ebay_listings.csv")

if __name__ == "__main__":
    scrape_ebay()

"""## 65 > PRICE < 150"""

import csv
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import NoSuchElementException, TimeoutException
import time
import random

user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/113.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/112.0.1722.64",
]

def get_driver():
    user_agent = random.choice(user_agents)
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument(f"user-agent={user_agent}")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    service = Service()
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def scrape_ebay():
    base_url = "https://www.ebay.com.my/b/Consumer-Electronics/293/bn_1865552?_pgn={}&_udhi=150&_udlo=65&mag=1&rt=nc"
    max_pages = 169
    driver = get_driver()
    all_data = []

    try:
        for page_num in range(1, max_pages + 1):
            url = base_url.format(page_num)
            print(f"\n--- Scraping Page {page_num} ---")
            driver.get(url)
            time.sleep(2 + random.uniform(0.5, 1.5))

            listings = driver.find_elements(By.CSS_SELECTOR, "div.brwrvr__item-card__signals")
            print(f"Found {len(listings)} listings")

            for listing in listings:
                try:
                    title = listing.find_element(By.CSS_SELECTOR, 'h3.textual-display.bsig__title__text').text
                except NoSuchElementException:
                    title = None

                try:
                    price = listing.find_element(By.CSS_SELECTOR, 'span.textual-display.bsig__price.bsig__price--displayprice').text
                except NoSuchElementException:
                    price = None

                try:
                    shippingfee = listing.find_element(By.CSS_SELECTOR, 'span.textual-display.bsig__generic.bsig__logisticsCost').text
                except NoSuchElementException:
                    shippingfee = None

                try:
                    conditions = listing.find_elements(By.CSS_SELECTOR, 'span.bsig__listingCondition > span')
                    texts = [c.text.strip() for c in conditions if c.text.strip() and c.text.strip() != "·"]
                except NoSuchElementException:
                    texts = []

                try:
                    link = listing.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')
                except NoSuchElementException:
                    link = None

                data = {
                    "title": title,
                    "price": price,
                    "shippingfee": shippingfee,
                    "condition": texts[0] if len(texts) > 0 else None,
                    "brand": texts[1] if len(texts) > 1 else None,
                    "link": link
                }

                all_data.append(data)

            time.sleep(random.uniform(2, 5))
    except TimeoutException as e:
        print(f"TimeoutException: {e}")
        driver.save_screenshot("timeout_error.png")
    except Exception as e:
        print(f"Unexpected error: {e}")
        driver.save_screenshot("general_error.png")
    finally:
        driver.quit()

        # Save to CSV
        keys = ["title", "price", "shippingfee", "condition", "brand", "link"]
        with open("ebay_listings.csv", "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            writer.writerows(all_data)
        print(f"\n✅ Data saved to ebay_listings.csv")

if __name__ == "__main__":
    scrape_ebay()

"""## PRICE > 150"""

import csv
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import NoSuchElementException, TimeoutException
import time
import random

user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/113.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/112.0.1722.64",
]

def get_driver():
    user_agent = random.choice(user_agents)
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument(f"user-agent={user_agent}")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    service = Service()
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def scrape_ebay():
    base_url = "https://www.ebay.com.my/b/Consumer-Electronics/293/bn_1865552?_pgn={}&_udlo=150&mag=1&rt=nc"
    max_pages = 169
    driver = get_driver()
    all_data = []

    try:
        for page_num in range(1, max_pages + 1):
            url = base_url.format(page_num)
            print(f"\n--- Scraping Page {page_num} ---")
            driver.get(url)
            time.sleep(2 + random.uniform(0.5, 1.5))

            listings = driver.find_elements(By.CSS_SELECTOR, "div.brwrvr__item-card__signals")
            print(f"Found {len(listings)} listings")

            for listing in listings:
                try:
                    title = listing.find_element(By.CSS_SELECTOR, 'h3.textual-display.bsig__title__text').text
                except NoSuchElementException:
                    title = None

                try:
                    price = listing.find_element(By.CSS_SELECTOR, 'span.textual-display.bsig__price.bsig__price--displayprice').text
                except NoSuchElementException:
                    price = None

                try:
                    shippingfee = listing.find_element(By.CSS_SELECTOR, 'span.textual-display.bsig__generic.bsig__logisticsCost').text
                except NoSuchElementException:
                    shippingfee = None

                try:
                    conditions = listing.find_elements(By.CSS_SELECTOR, 'span.bsig__listingCondition > span')
                    texts = [c.text.strip() for c in conditions if c.text.strip() and c.text.strip() != "·"]
                except NoSuchElementException:
                    texts = []

                try:
                    link = listing.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')
                except NoSuchElementException:
                    link = None

                data = {
                    "title": title,
                    "price": price,
                    "shippingfee": shippingfee,
                    "condition": texts[0] if len(texts) > 0 else None,
                    "brand": texts[1] if len(texts) > 1 else None,
                    "link": link
                }

                all_data.append(data)

            time.sleep(random.uniform(2, 5))
    except TimeoutException as e:
        print(f"TimeoutException: {e}")
        driver.save_screenshot("timeout_error.png")
    except Exception as e:
        print(f"Unexpected error: {e}")
        driver.save_screenshot("general_error.png")
    finally:
        driver.quit()

        # Save to CSV
        keys = ["title", "price", "shippingfee", "condition", "brand", "link"]
        with open("ebay_listings.csv", "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            writer.writerows(all_data)
        print(f"\n✅ Data saved to ebay_listings.csv")

if __name__ == "__main__":
    scrape_ebay()
